---
description: Run TimeGPT distributedly on top of Ray
output-file: computing_at_scale_ray_distributed.html
title: Ray
---


[Ray](https://www.ray.io/) is an open source unified compute framework
to scale Python workloads. In this guide, we will explain how to use
`TimeGPT` on top of Ray.

**Outline:**

1.  [Installation](#installation)

2.  [Load Your Data](#load-your-data)

3.  [Initialize Ray](#initialize-ray)

4.  [Use TimeGPT on Ray](#use-timegpt-on-ray)

5.  [Shutdown Ray](#shutdown-ray)

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nixtla/nixtla/blob/main/nbs/docs/tutorials/18_computing_at_scale_ray_distributed.ipynb)

## 1. Installation

Install Ray through [Fugue](https://fugue-tutorials.readthedocs.io/).
Fugue provides an easy-to-use interface for distributed computing that
lets users execute Python code on top of several distributed computing
frameworks, including Ray.

If executing on a distributed `Ray` cluster, ensure that the `nixtla`
library is installed across all the workers.

## 2. Load Data

You can load your data as a `pandas` DataFrame. In this tutorial, we
will use a dataset that contains hourly electricity prices from
different markets.

```python
import pandas as pd 

df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short.csv') 
df.head()
```

|     | unique_id | ds                  | y     |
|-----|-----------|---------------------|-------|
| 0   | BE        | 2016-10-22 00:00:00 | 70.00 |
| 1   | BE        | 2016-10-22 01:00:00 | 37.10 |
| 2   | BE        | 2016-10-22 02:00:00 | 37.10 |
| 3   | BE        | 2016-10-22 03:00:00 | 44.75 |
| 4   | BE        | 2016-10-22 04:00:00 | 37.10 |

## 3. Initialize Ray

Initialize `Ray` and convert the pandas DataFrame to a `Ray` DataFrame.

```python
import ray
from ray.cluster_utils import Cluster

ray_cluster = Cluster(
    initialize_head=True,
    head_node_args={"num_cpus": 2}
)
ray.init(address=ray_cluster.address, ignore_reinit_error=True)
```

``` text
2024-04-26 14:47:26,225 WARNING cluster_utils.py:121 -- Ray cluster mode is currently experimental and untested on Windows. If you are using it and running into issues please file a report at https://github.com/ray-project/ray/issues.
2024-04-26 14:47:28,636 INFO utils.py:108 -- Overwriting previous Ray address (127.0.0.1:64941). Running ray.init() on this node will now connect to the new instance at 127.0.0.1:60031. To override this behavior, pass address=127.0.0.1:64941 to ray.init().
2024-04-26 14:47:28,636 INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 127.0.0.1:60031...
2024-04-26 14:47:28,647 INFO worker.py:1621 -- Connected to Ray cluster.
```

|                     |             |
|:--------------------|:------------|
| **Python version:** | **3.10.14** |
| **Ray version:**    | **2.6.2**   |

```python
ray_df = ray.data.from_pandas(df)
ray_df
```

``` text
MaterializedDataset(
   num_blocks=1,
   num_rows=8400,
   schema={unique_id: object, ds: object, y: float64}
)
```

## 4. Use TimeGPT on Ray

Using `TimeGPT` on top of `Ray` is almost identical to the
non-distributed case. The only difference is that you need to use a
`Ray` DataFrame.

First, instantiate the `NixtlaClient` class.

```python
from nixtla import NixtlaClient
```


```python
nixtla_client = NixtlaClient(
    # defaults to os.environ.get("NIXTLA_API_KEY")
    api_key = 'my_api_key_provided_by_nixtla'
)
```

Then use any method from the `NixtlaClient` class such as
[`forecast`](https://nixtlaverse.nixtla.io/nixtla/nixtla_client.html#nixtlaclient-forecast)
or
[`cross_validation`](https://nixtlaverse.nixtla.io/nixtla/nixtla_client.html#nixtlaclient-cross-validation).

```python
fcst_df = nixtla_client.forecast(ray_df, h=12)
```

``` text
2024-04-26 14:47:31,617 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]
2024-04-26 14:47:31,618 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:31,619 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2024-04-26 14:47:32,114 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_coarse_key)] -> LimitOperator[limit=1]
2024-04-26 14:47:32,114 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:32,115 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2024-04-26 14:47:32,166 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_coarse_key)] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)]
2024-04-26 14:47:32,167 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:32,167 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
```

To visualize the result, use the `to_pandas` method to convert the
output of `Ray` to a `pandas` DataFrame.

```python
fcst_df.to_pandas().tail()
```

|     | unique_id | ds                  | TimeGPT   |
|-----|-----------|---------------------|-----------|
| 55  | NP        | 2018-12-24 07:00:00 | 53.784847 |
| 56  | NP        | 2018-12-24 08:00:00 | 54.437321 |
| 57  | NP        | 2018-12-24 09:00:00 | 54.66077  |
| 58  | NP        | 2018-12-24 10:00:00 | 54.744473 |
| 59  | NP        | 2018-12-24 11:00:00 | 54.737762 |

```python
cv_df = nixtla_client.cross_validation(ray_df, h=12, freq='H', n_windows=5, step_size=2)
```

``` text
2024-04-26 14:47:40,202 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]
2024-04-26 14:47:40,202 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:40,202 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2024-04-26 14:47:40,261 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_coarse_key)] -> LimitOperator[limit=1]
2024-04-26 14:47:40,261 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:40,262 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2024-04-26 14:47:40,305 INFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_coarse_key)] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)]
2024-04-26 14:47:40,306 INFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)
2024-04-26 14:47:40,306 INFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
```

```python
cv_df.to_pandas().tail()
```

``` text
(MapBatches(group_fn) pid=12096) INFO:nixtla.nixtla_client:Validating inputs...
```

|     | unique_id | ds                  | cutoff              | TimeGPT   |
|-----|-----------|---------------------|---------------------|-----------|
| 295 | NP        | 2018-12-23 19:00:00 | 2018-12-23 11:00:00 | 53.441555 |
| 296 | NP        | 2018-12-23 20:00:00 | 2018-12-23 11:00:00 | 52.649628 |
| 297 | NP        | 2018-12-23 21:00:00 | 2018-12-23 11:00:00 | 51.753975 |
| 298 | NP        | 2018-12-23 22:00:00 | 2018-12-23 11:00:00 | 50.681946 |
| 299 | NP        | 2018-12-23 23:00:00 | 2018-12-23 11:00:00 | 49.716431 |

You can also use exogenous variables with `TimeGPT` on top of `Ray`. To
do this, please refer to the [Exogenous
Variables](https://nixtlaverse.nixtla.io/nixtla/docs/tutorials/exogenous_variables.html)
tutorial. Just keep in mind that instead of using a pandas DataFrame,
you need to use a `Ray` DataFrame instead.

## 5. Shutdown Ray

When you are done, shutdown the `Ray` session.

```python
ray.shutdown()
```

