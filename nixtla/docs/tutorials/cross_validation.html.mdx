---
output-file: cross_validation.html
title: Cross Validation
---


One of the primary challenges in time series forecasting is the inherent
uncertainty and variability over time, making it crucial to validate the
accuracy and reliability of the models employed. Cross-validation, a
robust model validation technique, is particularly adapted for this
task, as it provides insights into the expected performance of a model
on unseen data, ensuring the forecasts are reliable and resilient before
being deployed in real-world scenarios.

`TimeGPT`, understanding the intricate needs of time series forecasting,
incorporates the `cross_validation` method, designed to streamline the
validation process for time series models. This functionality enables
practitioners to rigorously test their forecasting models against
historical data, assessing their effectiveness while tuning them for
optimal performance. This tutorial will guide you through the nuanced
process of conducting cross-validation within the `NixtlaClient` class,
ensuring your time series forecasting models are not just
well-constructed, but also validated for trustworthiness and precision.

```python
import pandas as pd
from nixtla import NixtlaClient
```


```python
nixtla_client = NixtlaClient(
    # defaults to os.environ.get("NIXTLA_API_KEY")
    api_key = 'my_api_key_provided_by_nixtla'
)
```

The `cross_validation` method within the `TimeGPT` class is an advanced
functionality crafted to perform systematic validation on time series
forecasting models. This method necessitates a dataframe comprising
time-ordered data and employs a rolling-window scheme to meticulously
evaluate the model’s performance across different time periods, thereby
ensuring the model’s reliability and stability over time.

Key parameters include `freq`, which denotes the data’s frequency and is
automatically inferred if not specified. The `id_col`, `time_col`, and
`target_col` parameters designate the respective columns for each
series’ identifier, time step, and target values. The method offers
customization through parameters like `n_windows`, indicating the number
of separate time windows on which the model is assessed, and
`step_size`, determining the gap between these windows. If `step_size`
is unspecified, it defaults to the forecast horizon `h`.

The process also allows for model refinement via `finetune_steps`,
specifying the number of iterations for model fine-tuning on new data.
Data pre-processing is manageable through `clean_ex_first`, deciding
whether to cleanse the exogenous signal prior to forecasting.
Additionally, the method supports enhanced feature engineering from time
data through the `date_features` parameter, which can automatically
generate crucial date-related features or accept custom functions for
bespoke feature creation. The `date_features_to_one_hot` parameter
further enables the transformation of categorical date features into a
format suitable for machine learning models.

In execution, `cross_validation` assesses the model’s forecasting
accuracy in each window, providing a robust view of the model’s
performance variability over time and potential overfitting. This
detailed evaluation ensures the forecasts generated are not only
accurate but also consistent across diverse temporal contexts.

```python
pm_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/peyton_manning.csv')
timegpt_cv_df = nixtla_client.cross_validation(
    pm_df, 
    h=7, 
    n_windows=5, 
    time_col='timestamp', 
    target_col='value', 
    freq='D',
)
timegpt_cv_df.head()
```


```python
from IPython.display import display
```


```python
cutoffs = timegpt_cv_df['cutoff'].unique()
for cutoff in cutoffs:
    fig = nixtla_client.plot(
        pm_df.tail(100), 
        timegpt_cv_df.query('cutoff == @cutoff').drop(columns=['cutoff', 'value']),
        time_col='timestamp', 
        target_col='value'
    )
    display(fig)
```

To asses the performance of `TimeGPT` with distributional forecasts, you
can produce prediction intervals using the `level` argument.

```python
timegpt_cv_df = nixtla_client.cross_validation(
    pm_df, 
    h=7, 
    n_windows=5, 
    time_col='timestamp', 
    target_col='value', 
    freq='D',
    level=[80, 90],
)
timegpt_cv_df.head()
```


```python
cutoffs = timegpt_cv_df['cutoff'].unique()
for cutoff in cutoffs:
    fig = nixtla_client.plot(
        pm_df.tail(100), 
        timegpt_cv_df.query('cutoff == @cutoff').drop(columns=['cutoff', 'value']),
        time_col='timestamp', 
        target_col='value',
        level=[80, 90],
        models=['TimeGPT']
    )
    display(fig)
```

You can also include `date_features` to see their impact in forecasting
accuracy:

```python
timegpt_cv_df = nixtla_client.cross_validation(
    pm_df, 
    h=7, 
    n_windows=5, 
    time_col='timestamp', 
    target_col='value', 
    freq='D',
    level=[80, 90],
    date_features=['month'],
)
timegpt_cv_df.head()
```


```python
cutoffs = timegpt_cv_df['cutoff'].unique()
for cutoff in cutoffs:
    fig = nixtla_client.plot(
        pm_df.tail(100), 
        timegpt_cv_df.query('cutoff == @cutoff').drop(columns=['cutoff', 'value']),
        time_col='timestamp', 
        target_col='value',
        level=[80, 90],
        models=['TimeGPT']
    )
    display(fig)
```

#### Exogenous variables

Additionally you can pass exogenous variables to better inform `TimeGPT`
about the data. You just simply have to add the exogenous regressors
after the target column.

```python
Y_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity.csv')
X_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/exogenous-vars-electricity.csv')
df = Y_df.merge(X_df)
```

Now let’s cross validate `TimeGPT` considering this information

```python
timegpt_cv_df_x = nixtla_client.cross_validation(
    df.groupby('unique_id').tail(100 * 48), 
    h=48, 
    n_windows=2,
    level=[80, 90]
)
cutoffs = timegpt_cv_df_x.query('unique_id == "BE"')['cutoff'].unique()
for cutoff in cutoffs:
    fig = nixtla_client.plot(
        df.query('unique_id == "BE"').tail(24 * 7), 
        timegpt_cv_df_x.query('cutoff == @cutoff & unique_id == "BE"').drop(columns=['cutoff', 'y']),
        models=['TimeGPT'],
        level=[80, 90],
    )
    display(fig)
```

#### Compare different models

Also, you can generate cross validation for different instances of
`TimeGPT` using the `model` argument.

```python
timegpt_cv_df_x_long_horizon = nixtla_client.cross_validation(
    df.groupby('unique_id').tail(100 * 48), 
    h=48, 
    n_windows=2,
    level=[80, 90],
    model='timegpt-1-long-horizon',
)
timegpt_cv_df_x_long_horizon.columns = timegpt_cv_df_x_long_horizon.columns.str.replace('TimeGPT', 'TimeGPT-LongHorizon')
timegpt_cv_df_x_models = timegpt_cv_df_x_long_horizon.merge(timegpt_cv_df_x)
cutoffs = timegpt_cv_df_x_models.query('unique_id == "BE"')['cutoff'].unique()
for cutoff in cutoffs:
    fig = nixtla_client.plot(
        df.query('unique_id == "BE"').tail(24 * 7), 
        timegpt_cv_df_x_models.query('cutoff == @cutoff & unique_id == "BE"').drop(columns=['cutoff', 'y']),
        models=['TimeGPT', 'TimeGPT-LongHorizon'],
        level=[80, 90],
    )
    display(fig)
```

